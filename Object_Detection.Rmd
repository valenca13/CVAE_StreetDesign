---
title: "Object Detection"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### 1. Install libraries

```{r message=FALSE, warning=FALSE}

#remotes::install_github("maju116/platypus")
```

##### 2. Import libraries
```{r message=FALSE, warning=FALSE}
# With TF-2, you can still run this code due to the following line:
if (tensorflow::tf$executing_eagerly())
  tensorflow::tf$compat$v1$disable_eager_execution()

library(keras)
K <- keras::backend()
library(tidyverse)
library(platypus)
library(abind)
#purrr::set_names 
```

##### 2. Import the "You Look Only Once" (YOLO) model for object detection

```{r}
test_yolo <- yolo3(
  net_h = 416, # Input image height. Must be divisible by 32
  net_w = 416, # Input image width. Must be divisible by 32
  grayscale = FALSE, # Images can be loaded as grayscale or RGB
  n_class = 80, # Number of object classes (80 for COCO dataset)
  anchors = coco_anchors)   # Anchor boxes


test_yolo
```
 
 * The first element of the train_pred is the number of images.  

 *  The Yolo network has 3 outputs: 
    - For large objects: (1:13, 1:13, 1:3);  
    
    - For medium objects: (1:26, 1:26, 1:3);  
    
    - For small objects: (1:52, 1:52, 1:52);  
    
 * The output objects are vectors of length 85.   
 
##### 3. [Download](https://pjreddie.com/darknet/yolo/) and import YOLOv3 Darknet weights trained on COCO dataset. 

```{r include=FALSE}
test_yolo %>% load_darknet_weights("C:/Users/gabri/Downloads/yolov3.weights")
```


```{r}
#test_yolo %>% load_darknet_weights("yolov3.weights")
```

>**Note**: The _yolov3.weights_ file is too large to be loaded to github. Therefore, you should download and run the code with the weights. 

##### 4. Calculate predictions for new images

```{r}
train_img_paths <- list.files("Example_Images",  full.names = TRUE, pattern = ".jpg", all.files = TRUE)

#train_img_paths <- list.files("Dataset/Train_filtered",  full.names = TRUE, pattern = ".jpg", all.files = TRUE)

train_imgs <- train_img_paths %>%
  map(~ {
    image_load(., target_size = c(416, 416), grayscale = FALSE) %>%
      image_to_array() %>%
      `/`(255)
  }) %>%
  abind(along=4) %>% #along = 4
  aperm(c(4, 1:3))

train_preds <- test_yolo %>% 
  predict(train_imgs) 
```

>**Note**: The same code is used for training and test data. The crucial point that needs to be changes is in importing the files. For training data use _"Dataset/Train_filtered"_ and for test data use _"Dataset/Test_filtered"_. Also we recommend adapting the name of variables according to the type of dataset.

##### 5. Transform raw predictions into bounding boxes:

```{r}
train_boxes <- get_boxes(
  preds = train_preds, # Raw predictions form YOLOv3 model
  anchors = coco_anchors, # Anchor boxes
  labels = coco_labels, # Class labels
  obj_threshold = 0.55, # Object threshold
  nms = TRUE, # Should non-max suppression be applied (ensures that objects are only detected once)
  nms_threshold = 0.6, # Non-max suppression threshold
  correct_hw = FALSE # Should height and width of bounding boxes be corrected to image height and width
)
```

##### 6. Define the labels

```{r}
labels <- array(train_boxes)

head(labels)
```

* Each row corresponds to an object detected in the image (bounding box).

##### 7. Convert the labels into different formats for testing in the Conditional Variational Autoencoder

* Extract the names of the images to match with the labels
```{r}
images_names = gsub(pattern = "\\.jpg$", "", basename(train_img_paths))
```

>Note: In the paper we tested only the format _"c"_. 

###### a) Tranform the labels in Yolo format and save in the Labels folder with the same name as the respective image.

```{r}
Exportlabels_txt = function(){
  for(i in 1:length(labels)) {
table_yolo = data.frame(labels[i]) %>%
  mutate(class_id = label_id,
         x_centre = mean(c(xmin, xmax)), 
         y_centre = mean(c(ymin, ymax)),
         width = xmax - xmin,
         height = ymax - ymin) %>% 
  select(class_id, x_centre, y_centre, width, height) #%>% 
  #write.table(file = paste0("Train/Labels/", images_names[i], ".txt"), sep = ",", row.names = FALSE, col.names = FALSE)   #Save labels in folder
}}
```

* In YOLO format, every image in the dataset has a single *.txt file.  

* YOLO format = [class_id, x_centre, y_centre, width, height].

###### b) Export labels with a single vector presenting the classes 


>**Note**: In the Coco dataset that the YOLOv3 was trained, the classes _Car_; _bus_; _truck_; _person_; _bicycle_; correspond to the following id: [3, 6, 8, 1, 2]. 
